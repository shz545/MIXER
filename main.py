#python MIXER/main.py
      
# æ¨¡çµ„è¼‰å…¥
import jax                                                        # JAX æ ¸å¿ƒåº«ï¼Œç”¨ä¾†æ§åˆ¶éš¨æ©Ÿæ€§å’ŒåŠ é€Ÿæ•¸å€¼é‹ç®—
import jax.numpy as jnp
from train import run_gga, train_with_config
from model import MlpMixer
import os
import numpy as np
import flax
import pickle
# é¡åˆ¥åç¨±
dataset_name = "mnist"  # âœ… å¯é¸ "cifar10" æˆ– "mnist"
if dataset_name == "cifar10":
    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
elif dataset_name == "mnist":
    classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

def save_param_to_mem(param, filename):
    arr = np.array(param)
    arr_q88 = np.round(arr * 256).astype(np.int16).flatten()
    with open(filename, "w") as f:
        for val in arr_q88:
            f.write(f"{np.uint16(val):04X}\n")

def export_all_params_q88(params, folder="kernel", prefix=""):
    os.makedirs(folder, exist_ok=True)
    for k, v in params.items():
        if isinstance(v, dict):
            export_all_params_q88(v, folder, prefix + k + "_")
        else:
            fname = f"{folder}/{prefix}{k}.mem"
            save_param_to_mem(v, fname)
            print(f"å·²å„²å­˜ {fname}ï¼Œshape={np.array(v).shape}")
            
def main():
    mode = "train"  # âœ… å¯é¸ "train" æˆ– "gga"
    trainornot = "y" # âœ… å¯é¸ "y" æˆ– "n"
    optimizer = "adamw" # âœ… å¯é¸ "adamw" æˆ– "sgd"
    earlystop = "n" # âœ… å¯é¸ "y" æˆ– "n"
    num_epochs = 50
    pop_size = 10
    generations = 10
    batch_size = 128

    if mode == "train":
        default_config = {
            "num_blocks": 3,
            "patch_size": 4,
            "hidden_dim": 128,
            "tokens_mlp_dim": 64,
            "channels_mlp_dim": 512,
            "dropout_rate": 0.1,
            "learning_rate": 0.001,
            "use_bn": False
        }
        
        train_with_config(default_config, num_epochs=num_epochs, batch_size=batch_size, earlystop=earlystop, dataset_name=dataset_name, optimizer=optimizer)
        
        model = MlpMixer(
            num_classes=10,
            num_blocks=default_config["num_blocks"],
            patch_size=default_config["patch_size"],
            hidden_dim=default_config["hidden_dim"],
            tokens_mlp_dim=default_config["tokens_mlp_dim"],
            channels_mlp_dim=default_config["channels_mlp_dim"],
            dropout_rate=default_config["dropout_rate"],
            use_bn=default_config["use_bn"]
        )
        dummy_input = jnp.ones((1, 32, 32, 3), dtype=jnp.float32)
        variables = model.init(jax.random.PRNGKey(0), dummy_input, False)
        params = variables["params"]
        export_all_params_q88(params)
        # å„²å­˜ Flax åƒæ•¸
        with open("mlp_mixer_params.pkl", "wb") as f:
            pickle.dump(params, f)
        print("å·²å„²å­˜ Flax è¨“ç·´åƒæ•¸åˆ° mlp_mixer_params.pkl")
        print(model.tabulate(
            jax.random.PRNGKey(0), 
            dummy_input, 
            False,
            depth=6,
            console_kwargs={"width": 200}
            ))

    elif mode == "gga":
        best_config = run_gga(pop_size=pop_size, generations=generations, dataset_name=dataset_name, optimizer=optimizer) #pop_size å€‹é«”æ•¸(éœ€>=2) , generations ä¸–ä»£æ•¸

        if trainornot == "y":
            print("\nğŸ¯ ä½¿ç”¨æœ€ä½³åƒæ•¸é€²è¡Œå®Œæ•´è¨“ç·´")
            train_with_config(best_config, num_epochs=num_epochs, batch_size=batch_size, earlystop=earlystop, dataset_name=dataset_name, optimizer=optimizer)
        else:
            print("\nğŸ¯ GGAçµæŸ ä¸é€²è¡Œå®Œæ•´è¨“ç·´")

if __name__ == "__main__":
    main()
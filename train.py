import jax
import jax.numpy as jnp
import optax
from flax.training import train_state

# ğŸ“˜ Cross-entropy loss function
def cross_entropy_loss(logits, labels):
    one_hot = jax.nn.one_hot(labels, num_classes=logits.shape[-1])
    return optax.softmax_cross_entropy(logits, one_hot).mean()

# ğŸ¯ Accuracy metric
def compute_accuracy(logits, labels):
    predictions = jnp.argmax(logits, axis=-1)
    return jnp.mean(predictions == labels)

# ğŸ“ˆ Learning rate schedule
def create_learning_rate_schedule(warmup_steps, total_steps, base_lr):
    return optax.warmup_cosine_decay_schedule(
        init_value=0.0,
        peak_value=base_lr,
        warmup_steps=warmup_steps,
        decay_steps=total_steps - warmup_steps,
        end_value=1e-5,
    )

# ğŸ—ï¸ Initialize TrainState with proper parameter creation
def create_train_state(rng, model, learning_rate, warmup_steps, total_steps):
    dummy_input = jnp.ones([1, 32, 32, 3])
    
    # åˆå§‹åŒ–è®Šæ•¸é›†åˆï¼ˆå¯èƒ½åŒ…å«params / batch_stats ç­‰ï¼‰
    variables = model.init(rng, dummy_input)

    # âœ¨ å¦‚æœéœ€è¦å‹•æ…‹å»ºç«‹åƒæ•¸ï¼ˆæ‡¶åˆå§‹åŒ–ï¼‰ï¼Œå¯åŸ·è¡Œä¸€æ¬¡ forward
    model.apply(variables, dummy_input)  # è‹¥ä½ æ²’ç”¨ BatchNorm / Dropoutï¼Œé€™è¡Œå°±å¤ 

    params = variables["params"]

    lr_schedule = create_learning_rate_schedule(warmup_steps, total_steps, learning_rate)
    tx = optax.adamw(learning_rate=lr_schedule)

    return train_state.TrainState.create(
        apply_fn=model.apply,
        params=params,
        tx=tx
    )

# ğŸ” Training step with JIT and gradient updates
@jax.jit
def train_step(state, batch):
    imgs, labels = batch

    def loss_fn(params):
        logits = state.apply_fn({'params': params}, imgs)
        loss = cross_entropy_loss(logits, labels)
        return loss, logits

    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
    (loss, logits), grads = grad_fn(state.params)
    state = state.apply_gradients(grads=grads)
    acc = compute_accuracy(logits, labels)
    
    return state, {
        'loss': loss,
        'accuracy': acc,
    }